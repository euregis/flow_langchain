import json
import os
import time
import aiofiles
from contextlib import asynccontextmanager
from async_lru import alru_cache # Vers√£o async do lru_cache
from typing import Dict, Any, List, Optional
from fastapi import FastAPI, HTTPException, Header, Body
from fastapi.responses import JSONResponse
import httpx
from langchain_openai import ChatOpenAI
from pydantic import BaseModel
from dotenv import load_dotenv
from py_expression_eval import Parser
from engine import FlowEngine
# from storage import InMemoryStore
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command

# Load environment variables
load_dotenv()

memory = MemorySaver()
@asynccontextmanager
async def lifespan(app: FastAPI):
    # --- INICIALIZA√á√ÉO (Roda 1 vez no boot) ---
    global global_llm, global_http_client, global_parser
    
    print("Criando recursos compartilhados...")
    global_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    global_http_client = httpx.AsyncClient() # Cria o pool de conex√£o
    global_parser = Parser()
    # global memory = MemorySaver()
    
    yield # A aplica√ß√£o roda aqui
    
    # --- LIMPEZA (Roda ao desligar) ---
    print("Fechando recursos...")
    await global_http_client.aclose()

app = FastAPI(title="Flow Execution API", lifespan=lifespan)
# store = InMemoryStore() 

# Vari√°veis globais ou Estado da Aplica√ß√£o
global_llm = None
global_http_client = None
global_parser = None
class Message(BaseModel):
    type: str
    content: Dict[str, Any]
    
# Request model for flow execution
class FlowExecutionRequest(BaseModel):
    messages: Optional[List[Message]] = {}

# CONFIGURA√á√ÉO DO CACHE
# maxsize=32: Guarda os √∫ltimos 32 arquivos na mem√≥ria.
# ttl=60: (Opcional no async-lru) Se quiser que o cache expire a cada 60s para pegar atualiza√ß√µes.
@alru_cache(maxsize=32)
async def get_flow_definition(flow_name_input: str):
    # 1. Normaliza√ß√£o do nome
    filename = flow_name_input if flow_name_input.endswith(".json") else f"{flow_name_input}.json"
    
    # 2. Estrat√©gia EAFP (Tenta abrir direto, trata erro se falhar)
    try:
        # Leitura n√£o bloqueante (libera a CPU para outras requests enquanto l√™ o disco)
        async with aiofiles.open(filename, mode='r') as f:
            content = await f.read()
            # O parse do JSON ainda √© CPU-bound, mas para arquivos < 1MB √© irrelevante
            return json.loads(content)
            
    except FileNotFoundError:
        # Cacheia o erro? Depende. Aqui optei por lan√ßar direto, 
        # mas cuidado: se o arquivo for criado depois, o cache pode lembrar que "n√£o existe".
        # Para evitar cachear erros, limpamos o cache dessa entrada espec√≠fica (avan√ßado).
        raise HTTPException(status_code=404, detail=f"Flow definition '{filename}' not found.")
        
    except json.JSONDecodeError:
        raise HTTPException(status_code=500, detail=f"Invalid JSON format in '{filename}'.")
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Unexpected error: {str(e)}")



@app.post("/execute/{flow_name}")
async def execute_flow(
    flow_name: str,
    request: FlowExecutionRequest,
    x_user_id: str = Header(..., description="Unique User ID for context isolation")
):
    """
    Executes a flow defined in a JSON file.
    
    - **flow_name**: The name of the flow file (e.g., flow_definition.json)
    - **inputs**: Optional dictionary of inputs to pass to the flow (mapped by node ID)
    - **x-user-id**: Header to identify the user/session
    """
    config = {"configurable": {"thread_id": x_user_id}}
    
    # --- No seu endpoint/bloco principal ---
    inicio = time.perf_counter()

    # Na primeira vez: l√™ do disco (lento). 
    # Nas pr√≥ximas: l√™ da RAM (instant√¢neo, < 0.00001s).
    flow_config = await get_flow_definition(flow_name) 

    fim = time.perf_counter()

    tempo_execucao = fim - inicio
    print(f"O c√≥digo levou {tempo_execucao:.5f} segundos para carregar json.")
    
    # 2. Initialize Engine with Context Isolation (using User ID)
    # Note: In a real persistent store, we would use x_user_id to fetch/save state.
    # For InMemoryStore, we create a fresh instance per request or could map it.
    # Given the requirement "separate context of concurrent calls", a fresh store per request 
    # or a store keyed by user_id is needed. Here we instantiate a fresh engine/store for the run.
    
    # Initialize Engine
    inicio = time.perf_counter()
    engine = FlowEngine(
        flow_config=flow_config, 
        user_id=x_user_id, 
        # store=store,
        memory=memory,
        llm=global_llm,              # J√° est√° pronto na mem√≥ria
        http_client=global_http_client, # Pool de conex√£o aberto
        parser=global_parser
    )
    
    fim = time.perf_counter()
    tempo_execucao = fim - inicio
    print(f"O c√≥digo levou {tempo_execucao:.5f} segundos para inicializar o engine.")
    
    # 3. Build the Graph
    try:
        inicio = time.perf_counter()
        flow_app = await engine.build_graph()
        fim = time.perf_counter()
        tempo_execucao = fim - inicio
        print(f"O c√≥digo levou {tempo_execucao:.5f} segundos para construir o grafo.")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error building flow graph: {str(e)}")
     
    # Defensive checks: assegura que build_graph retornou um objeto utiliz√°vel
    if flow_app is None:
        raise HTTPException(status_code=500, detail="Flow engine returned None when building the graph.")
    if not hasattr(flow_app, "astream") or not callable(getattr(flow_app, "astream")):
        raise HTTPException(status_code=500, detail="Built flow app does not expose an async 'astream' method.")
    
    # 4. Prepare Initial State
    snapshot = await flow_app.aget_state(config)
    
    state = None
    initial_state = {
        "context": {
            "user_id": x_user_id # Inject user_id into context if needed by nodes
        },
        "current_node": flow_config["nodes"][0]["id"]
    }
    
    if snapshot.next:
        print(f"üîÑ Retomando sess√£o {x_user_id}...")
        # Criamos o comando de resume com a mensagem do usu√°rio
        state = Command(resume=request.messages)
        # valor_resume = None
        # if request.messages and len(request.messages) > 0:
        #     # Assume que quer o texto da primeira mensagem enviada
        #     primeira_msg = request.messages[0]
        #     # Acessa o dict 'content' e pega a chave 'text'
        #     if isinstance(primeira_msg.content, dict):
        #         valor_resume = primeira_msg.content.get("text")
        #     else:
        #         valor_resume = primeira_msg.content # Fallback
        
        # # Se n√£o enviou nada, usa None ou trata erro
        # if valor_resume is None:
        #      raise HTTPException(status_code=400, detail="Nenhum texto encontrado na mensagem para resume.")

        # # Agora enviamos apenas a STRING "pikachu"
        # state = Command(resume=valor_resume)
    
    # Cen√°rio B: O workflow n√£o existe ou j√° terminou
    else:
        print(f"‚ñ∂Ô∏è Iniciando nova sess√£o {x_user_id}...")
        # Criamos o input inicial padr√£o
        state = initial_state
    # state = store.get_state(x_user_id) or initial_state
    # Ensure context dict exists and inject inputs into context
    # state.setdefault("context", {})
    # state["context"]["user_inputs"] = request.messages
    
    
    print(f"[{x_user_id}] Starting flow '{flow_name}' with inputs: {request.messages}")

    # 5. Execute Flow
    try:
        # We use ainvoke to run until completion and get the final state
        # final_state = await flow_app.ainvoke(initial_state)
        status = "running"
        final_context = {}
        final_message = None

        async for output in flow_app.astream(state, config):
            print(f"[{x_user_id}] Flow step output: {output}, flow app: {flow_app}")

            # Normalize output shape. Some nodes emit: {"node_id": { ... }}
            # while others may emit the inner dict directly.
            inner = None
            if isinstance(output, dict) and len(output) == 1:
                key = next(iter(output))
                val = output[key]
                if isinstance(val, dict):
                    inner = val
                    inner["_node_id"] = key

            if inner is None:
                inner = output if isinstance(output, dict) else {}

            # store.save_state(x_user_id, inner)
            # Extract fields with fallbacks to previous values
            status = inner.get("status", status)
            final_context = inner.get("context", final_context)
            final_message = final_context.get("final_message", final_message)

            # Some flows put status inside the context dict (e.g., context['status'])
            if isinstance(final_context, dict) and final_context.get("status"):
                status = final_context.get("status", status)

            # If the flow is waiting for input, return current state to caller
            # if status == "waiting_input":
            #     return JSONResponse(
            #         content={
            #             "status": status,
            #             "message": final_message,
            #         },
            #         media_type="application/json; charset=utf-8"
            #     )
    
        # Extract relevant results
        
        snapshot_final = await flow_app.aget_state(config)
        
        if snapshot_final.next:
            # Acessamos a informa√ß√£o do interrupt
            # Geralmente √© a primeira tarefa da lista
            tarefa_atual = snapshot_final.tasks[0]
            
            # O valor passado dentro da fun√ß√£o interrupt("Mensagem") est√° aqui:
            mensagem_interrupt = tarefa_atual.interrupts[0].value
            
            return JSONResponse(
                content={
                    "status": "waiting_input",
                    "message": mensagem_interrupt,
                },
                media_type="application/json; charset=utf-8"
            )
            
        # CEN√ÅRIO B: O grafo terminou todo o processo
        else:
            # Aqui voc√™ pega o resultado final do state, se houver
            # resposta_api["status_workflow"] = "finalizado"
            # Ex: resposta_api["resultado"] = snapshot_final.values.get("context")
        
            return JSONResponse(
                content={
                    "status": status,
                    "message": final_message,
                },
                media_type="application/json; charset=utf-8"
            )
        
    except Exception as e:
        print(f"[{x_user_id}] Error executing flow: {e}")
        raise HTTPException(status_code=500, detail=f"Error executing flow: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
